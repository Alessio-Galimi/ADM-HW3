{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd       # importing the libraries we will need\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this cell, we create the list of URLs. Some of the next cells have the # before each line because there is no need to execute that cell more than once  #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#number_of_pages=300\n",
    "#for page in range(1,number_of_pages+1):\n",
    "    #url=\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+str(page)\n",
    "    #cnt = requests.get(url)\n",
    "    #soup = BeautifulSoup(cnt.content, features=\"lxml\")\n",
    "    #links = soup.find_all('a',href=True)\n",
    "    #f = open(\"book_list.txt\",'a')\n",
    "    #n=0\n",
    "    #for link in links:\n",
    "        #fullLink = link.get('href')\n",
    "        #if fullLink[0:11]==\"/book/show/\":\n",
    "            #n=n+1\n",
    "            #if n%2==0:\n",
    "                #fullLink=\"https://www.goodreads.com/\"+fullLink+\"\\n\"\n",
    "                #f.write(fullLink)\n",
    "    #f.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install selenium\n",
    "#!pip3 install webdriver_manager\n",
    "#from selenium import webdriver\n",
    "#from selenium.webdriver.common.keys import Keys\n",
    "#from webdriver_manager.firefox import GeckoDriverManager\n",
    "#driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this cell, we get the html file relative to each URL.  #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#f=open(\"book_list.txt\",\"r\")\n",
    "#n=0 \n",
    "#done=29648\n",
    "#for url in f:\n",
    "    #n=n+1\n",
    "    #if n>done:\n",
    "        #driver.get(url)\n",
    "        #x=driver.page_source\n",
    "        #f = open(\"book\"+str(n)+\".html\", \"w\",encoding=\"utf-8\")\n",
    "        #f.write(x)\n",
    "        #f.close()\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this cell, we create a function which analyzes every html file and returns the features we are looking for. For some books, some features may be missing, so we used the try-except statement in order to avoid errors #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_book(html):\n",
    "    page_soup =BeautifulSoup(html,\"html.parser\") \n",
    "    try:\n",
    "        bookTitle = page_soup.find_all('h1')[0].contents[0].replace('\\n', '').strip()\n",
    "    except:\n",
    "        bookTitle=\"\"\n",
    "    try:\n",
    "        bookSeries = page_soup.find(\"h2\", id=\"bookSeries\")\n",
    "        bookSeries=str(bookSeries).split(\"\\n\")[2].strip()\n",
    "    except:\n",
    "        bookSeries=\"\" \n",
    "    try:\n",
    "        bookAuthors = page_soup.find_all('span', itemprop='name')[0].contents[0]\n",
    "    except: \n",
    "        bookAuthors=\"\"\n",
    "    try:\n",
    "        ratingValue = page_soup.find_all('span', itemprop='ratingValue')[0].contents[0].strip()\n",
    "    except:\n",
    "        ratingValue=\"\"\n",
    "    try:\n",
    "        NumberofPages = page_soup.find_all('span', itemprop='numberOfPages')[0].contents[0].split(\" \")[0]\n",
    "    except:\n",
    "        NumberofPages=\"\"\n",
    "    try:\n",
    "        Plot = ' '.join([c for c in page_soup.find_all('div', id=\"description\")[0].contents[1].contents if isinstance(c, str)])\n",
    "    except:\n",
    "        Plot=\"\"\n",
    "    try:\n",
    "        Characters = []\n",
    "        for a in page_soup.find_all('a', href=True): \n",
    "            if a is not None:\n",
    "                b=str(a)\n",
    "                if \"/characters/\" in b:\n",
    "                    Ch_name=str()\n",
    "                    c=a['href']\n",
    "                    d=c.split(\"-\")\n",
    "                    d=d[1:]\n",
    "                    for i in d:\n",
    "                        Ch_name=Ch_name+i+\" \"\n",
    "                    Characters.append(Ch_name)\n",
    "    except:\n",
    "        Characters=\"\"\n",
    "    try:\n",
    "        Setting = []\n",
    "        for a in page_soup.find_all('a', href=True): \n",
    "            if a is not None:\n",
    "                b=str(a)\n",
    "                if \"/places/\" in b:\n",
    "                    Setting_name=str()\n",
    "                    c=a['href']\n",
    "                    d=c.split(\"-\")\n",
    "                    d=d[1:]\n",
    "                    for i in d:\n",
    "                        Setting_name=Setting_name+i+\" \"\n",
    "                    Setting.append(Setting_name)\n",
    "    except:\n",
    "        Setting=\"\"\n",
    "    try:\n",
    "        Url=page_soup.find('link', href=True,rel=\"canonical\")[\"href\"]\n",
    "    except:\n",
    "        Url=\"\"\n",
    "    try:\n",
    "        ratings = page_soup.find_all('a', href=\"#other_reviews\")\n",
    "        ratingCount = -1\n",
    "        reviewCount = -1\n",
    "        for raiting in ratings:\n",
    "            if raiting.find_all('meta', itemprop=\"ratingCount\"):\n",
    "                ratingCount = raiting.text.replace('\\n', '').strip().split(' ')[0]\n",
    "            elif raiting.find_all('meta', itemprop=\"reviewCount\"):\n",
    "                reviewCount = raiting.text.replace('\\n', '').strip().split(' ')[0]\n",
    "    except:\n",
    "        ratingCount=\"\"\n",
    "        reviewCount=\"\"\n",
    "    return (bookTitle, bookSeries, bookAuthors,ratingValue, ratingCount, reviewCount,Plot,NumberofPages,Characters,Setting,Url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this cell, we apply the scrap_book function to every book. The results are saved in a .tsv file. In this cell, we forgot to write the line relative to the indexes. We will solve this problem later #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for i in range(1,30001):\n",
    "    #try:\n",
    "        #s=\"html/book\"+str(i)+\".html\"\n",
    "        #book=open(s,\"r\",encoding=\"utf-8\")\n",
    "        #html=book.read()\n",
    "        #scrap_book(html)\n",
    "        #results=scrap_book(html)\n",
    "        #toWrite=\"\"\n",
    "        #for elem in results:\n",
    "            #toWrite=toWrite+str(elem)+\" \\t \"\n",
    "        #name=\"article\"+str(i)+\".tsv\"\n",
    "        #f =open(name,'w',encoding=\"utf-8\")\n",
    "        #f.write(toWrite)\n",
    "        #f.close()\n",
    "        #book.close()\n",
    "    #except:\n",
    "        #pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this cell, we remove the books which have the plot written in a language different than English. We also realize that we forgot to write the indexes in the tsv files, so the data became the indexes. We are still able to use the data correctly, but we want to add the indexes too. So we fix this problem in the next cell. #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(1,30001):\n",
    "    #try:\n",
    "        #s=\"tsv/article\"+str(i)+\".tsv\"\n",
    "        #book=pd.read_csv(s,sep=\"\\t\")\n",
    "        #if detect(book.columns[6]) != \"en\":\n",
    "            #os.remove(s)\n",
    "    #except:\n",
    "        #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this cell, we write new .tsv files where the indexes are included. We also delete the old .tsv files #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(1,30001):\n",
    "    #try:\n",
    "        #s=\"tsv/article\"+str(i)+\".tsv\"\n",
    "        #book=open(s,\"r\",encoding=\"utf-8\")\n",
    "        #data=book.read()\n",
    "        #book.close()\n",
    "        #s2=\"tsv/article_\"+str(i)+\".tsv\"\n",
    "        #f=open(s2,'w',encoding=\"utf-8\")\n",
    "        #f.write(\"bookTitle\\tbookSeries\\tbookAuthors\\tratingValue\\tratingCount\\treviewCount\\tPlot\\tNumberofPages\\tCharacters\\tSetting\\tUrl\\t\\n\")\n",
    "        #f.write(data)\n",
    "        #f.close()\n",
    "        #os.remove(s)\n",
    "    #except:\n",
    "        #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\aless_oh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip3 install nltk\n",
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this cell, I create the vocabulary.csv file, in which every word will be mapped to an integer \"term_id\" #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(\"vocabulary.csv\",'w',encoding=\"utf-8\")\n",
    "#f.write(\"term_id,word\\n\")\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this cell, we look for the words contained in the plot of each book. We remove from the plot the punctuation and the stopwords. We also stem the words ##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set_of_words=set()\n",
    "#for i in range(1,30001):\n",
    "    #try:\n",
    "        #s=\"tsv/article_\"+str(i)+\".tsv\"\n",
    "        #book=pd.read_csv(s,sep=\"\\t\")\n",
    "        #data=book[\"Plot\"][0]\n",
    "        #words = word_tokenize(data)\n",
    "        #words = [x.lower() for x in words if x.isalpha()]\n",
    "        #stop_words = set(stopwords.words('english'))\n",
    "        #words = [x for x in words if not x in stop_words]\n",
    "        #porter = PorterStemmer()\n",
    "        #words = [porter.stem(x) for x in words]\n",
    "        #for elem in words:\n",
    "            #set_of_words.add(elem)\n",
    "    #except:\n",
    "        #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this cell, we add every word with the relative term_id to the vocabulary\" #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n=0\n",
    "#for i in set_of_words:\n",
    "    #n=n+1\n",
    "    #f = open(\"vocabulary.csv\",'a',encoding=\"utf-8\")\n",
    "    #new_line=str(n)+\", \"+i+\"\\n\"\n",
    "    #f.write(new_line)\n",
    "    #f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the next cells, we create the Inverted Index. It's a dictionary where the keys are the term_id, and the value of each key is a list of the id of each document whose plot contains all the searched words. We will save this dictionary in a .json file. #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inv_index={}\n",
    "#for i in range (1,30001):\n",
    "    #try:\n",
    "        #s=\"tsv/article_\"+str(i)+\".tsv\"\n",
    "        #set_of_words=set()\n",
    "        #book=pd.read_csv(s,sep=\"\\t\")\n",
    "        #data=book[\"Plot\"][0]\n",
    "        #words = word_tokenize(data)\n",
    "        #words = [x.lower() for x in words if x.isalpha()]\n",
    "        #stop_words = set(stopwords.words('english'))\n",
    "        #words = [x for x in words if not x in stop_words]\n",
    "        #porter = PorterStemmer()\n",
    "        #words = [porter.stem(x) for x in words]\n",
    "        #for elem in words:\n",
    "            #set_of_words.add(elem)\n",
    "        #for y in set_of_words:\n",
    "            #if y in inv_index:\n",
    "                #inv_index[y].append(s)\n",
    "            #else:\n",
    "                #inv_index[y]=[s]\n",
    "    #except:\n",
    "        #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"vocabulary.csv\")\n",
    "#for index,row in data.iterrows():\n",
    "    #numerical_key=row[\"term_id\"]\n",
    "    #word_as_key=row[\"word\"].strip()\n",
    "    #inv_index[numerical_key] = inv_index.pop(word_as_key.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#json.dump(inv_index,open(\"inv_index.json\",'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this cell, we ask the user to write the words he is looking for. After executing the next cells, the user will receive as a result a dataframa containing some informations about the books whose plot contains all the words he looked for. #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write the words you are looking for:katniss, games\n"
     ]
    }
   ],
   "source": [
    "x=input(\"write the words you are looking for:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_of_words=set()\n",
    "words = word_tokenize(x)\n",
    "words = [x.lower() for x in words if x.isalpha()]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [x for x in words if not x in stop_words]\n",
    "porter = PorterStemmer()\n",
    "words = [porter.stem(x) for x in words]\n",
    "for elem in words:\n",
    "    set_of_words.add(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8713, 27386]\n"
     ]
    }
   ],
   "source": [
    "term_id_list=list()\n",
    "for i in set_of_words:\n",
    "    data = pd.read_csv(\"vocabulary.csv\")\n",
    "    for index,row in data.iterrows():\n",
    "        if row[\"word;\"].strip(\" ;\")==i:\n",
    "            term_id_list.append(row[\"term_id\"])\n",
    "print(term_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"inv_index.json\")\n",
    "inv_index=json.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tsv/article_223.tsv', 'tsv/article_325.tsv', 'tsv/article_5288.tsv'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books=set(inv_index[str(term_id_list[0])])\n",
    "if len(term_id_list)>1:\n",
    "    for i in term_id_list[1:]:\n",
    "        key=str(i)\n",
    "        new_set=set(inv_index[key])\n",
    "        books=books.intersection(new_set)\n",
    "books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we can see the result of the search #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookTitle</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SAMPLER ONLY: Catching Fire (The Hunger Games,...</td>\n",
       "      <td>Against all odds, Katniss Everdeen has won th...</td>\n",
       "      <td>https://www.goodreads.com/book/show/20349441-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mockingjay</td>\n",
       "      <td>The final book in the ground-breaking HUNGER ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/7260188-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Catching Fire</td>\n",
       "      <td>Against all odds, Katniss Everdeen has surviv...</td>\n",
       "      <td>https://www.goodreads.com/book/show/6148028-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           bookTitle  \\\n",
       "0  SAMPLER ONLY: Catching Fire (The Hunger Games,...   \n",
       "1                                        Mockingjay    \n",
       "2                                     Catching Fire    \n",
       "\n",
       "                                                Plot  \\\n",
       "0   Against all odds, Katniss Everdeen has won th...   \n",
       "1   The final book in the ground-breaking HUNGER ...   \n",
       "2   Against all odds, Katniss Everdeen has surviv...   \n",
       "\n",
       "                                                 Url  \n",
       "0   https://www.goodreads.com/book/show/20349441-...  \n",
       "1   https://www.goodreads.com/book/show/7260188-m...  \n",
       "2   https://www.goodreads.com/book/show/6148028-c...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_list=list()\n",
    "plot_list=list()\n",
    "url_list=list()\n",
    "for i in books:\n",
    "    book=pd.read_csv(i,sep=\"\\t\")\n",
    "    title_list.append(book[\"bookTitle\"][0])\n",
    "    plot_list.append(book[\"Plot\"][0])\n",
    "    url_list.append(book[\"Url\"][0])\n",
    "result=pd.DataFrame({\"bookTitle\":title_list,\"Plot\":plot_list,\"Url\":url_list})\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
